{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a0a3a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'GEMINI_API_KEY'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f745eed6edac>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mAPI_KEY\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"GEMINI_API_KEY\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mAPI_KEY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'set AIzaSyCaHpmgh4VqW9fUOMdVa6pv7shxElHm8dk'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GEMINI_API_KEY\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# should be True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Pratishtha Singh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    677\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m             \u001b[1;31m# raise KeyError with the original key value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 679\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    680\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'GEMINI_API_KEY'"
     ]
    }
   ],
   "source": [
    "# pip install -U google-genai\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import json\n",
    "\n",
    "import os\n",
    "API_KEY=os.environ[\"GEMINI_API_KEY\"] \n",
    "assert API_KEY, 'set api_key'\n",
    "print(bool(os.getenv(\"GEMINI_API_KEY\")))  # should be True\n",
    "client = genai.Client(api_key=API_KEY)\n",
    "\n",
    "ENDPOINT = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-001:generateContent\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a humor-perception classifier for a single individual.\n",
    "\n",
    "Labels (choose exactly one):\n",
    "- funny            : the person likely understands it AND finds it amusing\n",
    "- not_funny        : the person likely understands it but does not find it amusing\n",
    "- dont_understand  : only if comprehension is unlikely due to language/cultural knowledge gaps, obscure references, or heavy wordplay that the person is unlikely to get\n",
    "\n",
    "Decision rules:\n",
    "1) FIRST assess comprehension. Use dont_understand ONLY when there are strong signals of likely confusion (unfamiliar slang, obscure cultural references for that demographic, heavy wordplay unlikely to translate). Ambiguity alone is NOT enough.\n",
    "2) If comprehension is plausible, DO NOT use dont_understand. Decide between funny vs not_funny.\n",
    "3) When uncertain between funny vs not_funny, prefer not_funny (conservative choice) rather than dont_understand.\n",
    "4) Keep dont_understand rare (roughly 5–15% in typical mixed audiences).\n",
    "5) Use ONLY the provided info; do not stereotype beyond it.\n",
    "\n",
    "Output JSON ONLY with:\n",
    "{\"label\":\"funny|not_funny|dont_understand\",\"confidence\":0.0-1.0,\"short_reason\":\"<=25 words\"}\n",
    "\n",
    "Few-shot guidance:\n",
    "Example A:\n",
    "Joke: \"I'm reading a book on anti-gravity — it's impossible to put down.\"\n",
    "Demographics: age 26-35, gender female, ethnicity South Asian\n",
    "Decision: {\"label\":\"funny\",\"confidence\":0.74,\"short_reason\":\"Understands pun on 'put down'; light, broadly relatable wordplay.\"}\n",
    "\n",
    "Example B:\n",
    "Joke: \"I told my wife she was drawing her eyebrows too high. She looked surprised.\"\n",
    "Demographics: age 36-45, gender male, ethnicity White/Caucasian\n",
    "Decision: {\"label\":\"funny\",\"confidence\":0.62,\"short_reason\":\"Understands 'surprised' visual pun; mild dry humor.\"}\n",
    "\n",
    "Example C:\n",
    "Joke: \"I prefer my puns intended.\"\n",
    "Demographics: age 18-25, gender female, ethnicity East Asian\n",
    "Decision: {\"label\":\"not_funny\",\"confidence\":0.60,\"short_reason\":\"Understands wordplay but likely not amusing for this person.\"}\n",
    "\n",
    "Example D:\n",
    "Joke: \"We queued for ages at the chippy—proper knackered now.\"\n",
    "Demographics: age 18-25, gender male, ethnicity North American\n",
    "Decision: {\"label\":\"dont_understand\",\"confidence\":0.78,\"short_reason\":\"UK-specific slang and context likely unfamiliar; comprehension unlikely.\"}\n",
    "\n",
    "Example E:\n",
    "Joke: \"The rotation in the 2011 Spurs' PnR was textbook—Pop would be proud.\"\n",
    "Demographics: age 55-64, gender female, ethnicity South Asian\n",
    "Decision: {\"label\":\"dont_understand\",\"confidence\":0.72,\"short_reason\":\"Niche NBA jargon/reference; low chance of comprehension.\"}\n",
    "\n",
    "Example F:\n",
    "Joke: \"Why did the scarecrow get a promotion? He was outstanding in his field.\"\n",
    "Demographics: age 46-55, gender male, ethnicity Middle Eastern\n",
    "Decision: {\"label\":\"not_funny\",\"confidence\":0.55,\"short_reason\":\"Understands pun but likely finds it stale.\"}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def _user_prompt(joke: str, age: str, gender: str, ethnicity: str) -> str:\n",
    "    return f\"\"\"Classify the joke for this individual.\n",
    "\n",
    "Joke:\n",
    "\\\"\\\"\\\"{joke.strip()}\\\"\\\"\\\"\n",
    "\n",
    "Demographics:\n",
    "- age: {age}\n",
    "- gender: {gender}\n",
    "- ethnicity: {ethnicity}\n",
    "\n",
    "Return only JSON with fields:\n",
    "- label ∈ [funny, not_funny, dont_understand]\n",
    "- confidence ∈ [0,1]\n",
    "- short_reason (<= 25 words)\n",
    "\"\"\"\n",
    "\n",
    "def classify_with_gemini_rest(\n",
    "    joke: str,\n",
    "    age: str,\n",
    "    gender: str,\n",
    "    ethnicity: str,\n",
    "    temperature: float = 0.0,\n",
    "    max_output_tokens: int = 128,\n",
    "    model_url: str = ENDPOINT,\n",
    "):\n",
    "    body = {\n",
    "        \"system_instruction\": {\"parts\": [{\"text\": SYSTEM_PROMPT}]},\n",
    "        \"contents\": [\n",
    "            {\"role\": \"user\", \"parts\": [{\"text\": _user_prompt(joke, age, gender, ethnicity)}]}\n",
    "        ],\n",
    "        \"generation_config\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"max_output_tokens\": max_output_tokens,\n",
    "            \"response_mime_type\": \"application/json\"\n",
    "        }\n",
    "    }\n",
    "    r = requests.post(f\"{model_url}?key={API_KEY}\", json=body, timeout=60)\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(f\"Gemini REST error {r.status_code}: {r.text[:600]}\")\n",
    "\n",
    "    data = r.json()\n",
    "    # If there are no candidates, the API says the prompt was blocked/invalid—surface promptFeedback. :contentReference[oaicite:3]{index=3}\n",
    "    cands = data.get(\"candidates\", [])\n",
    "    if not cands:\n",
    "        pf = data.get(\"promptFeedback\", {})\n",
    "        raise RuntimeError(f\"No candidates. promptFeedback={pf}\")\n",
    "\n",
    "    # Extract first text part\n",
    "    parts = cands[0].get(\"content\", {}).get(\"parts\", [])\n",
    "    text = \"\"\n",
    "    for p in parts:\n",
    "        t = p.get(\"text\")\n",
    "        if t:\n",
    "            text = t\n",
    "            break\n",
    "    if not text:\n",
    "        raise RuntimeError(f\"No text in first candidate: {cands[0]}\")\n",
    "\n",
    "    # Parse JSON (strip code fences if any)\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        if text.startswith(\"```\"):\n",
    "            text = text.strip(\"`\")\n",
    "            if \"\\n\" in text: text = text.split(\"\\n\", 1)[-1]\n",
    "            text = text.strip()\n",
    "        return json.loads(text)\n",
    "\n",
    "# --- Example ---\n",
    "if __name__ == \"__main__\":\n",
    "    out = classify_with_gemini_rest(\n",
    "        joke=\"I'm on a whiskey diet — I've lost three days already.\",\n",
    "        age=\"26-35\", gender=\"female\", ethnicity=\"South Asian\"\n",
    "    )\n",
    "    print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d4fccb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes -> train: 1071 | val: 153 | test: 306\n",
      "\n",
      "=== Evaluating on Validation set ===\n",
      "Processed 25/153 rows\n",
      "Processed 50/153 rows\n",
      "Processed 75/153 rows\n",
      "Processed 100/153 rows\n",
      "Processed 125/153 rows\n",
      "Processed 150/153 rows\n",
      "\n",
      "Confusion Matrix (rows=true, cols=pred):\n",
      "                 funny  not_funny  dont_understand\n",
      "funny                9          1               36\n",
      "not_funny           12          9               75\n",
      "dont_understand      1          0               10\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "          funny     0.4091    0.1957    0.2647        46\n",
      "      not_funny     0.9000    0.0938    0.1698        96\n",
      "dont_understand     0.0826    0.9091    0.1515        11\n",
      "\n",
      "       accuracy                         0.1830       153\n",
      "      macro avg     0.4639    0.3995    0.1953       153\n",
      "   weighted avg     0.6936    0.1830    0.1970       153\n",
      "\n",
      "\n",
      "Validation Macro-F1: 0.1953\n",
      "\n",
      "=== Evaluating on Test set ===\n",
      "Processed 25/306 rows\n",
      "Processed 50/306 rows\n",
      "Processed 75/306 rows\n",
      "Processed 100/306 rows\n",
      "Processed 125/306 rows\n",
      "Processed 150/306 rows\n",
      "Processed 175/306 rows\n",
      "Processed 200/306 rows\n",
      "Processed 225/306 rows\n",
      "Processed 250/306 rows\n",
      "Processed 275/306 rows\n",
      "Processed 300/306 rows\n",
      "\n",
      "Confusion Matrix (rows=true, cols=pred):\n",
      "                 funny  not_funny  dont_understand\n",
      "funny               11          0               82\n",
      "not_funny           10          4              177\n",
      "dont_understand      2          0               20\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "          funny     0.4783    0.1183    0.1897        93\n",
      "      not_funny     1.0000    0.0209    0.0410       191\n",
      "dont_understand     0.0717    0.9091    0.1329        22\n",
      "\n",
      "       accuracy                         0.1144       306\n",
      "      macro avg     0.5166    0.3494    0.1212       306\n",
      "   weighted avg     0.7747    0.1144    0.0928       306\n",
      "\n",
      "\n",
      "Test Macro-F1: 0.1212\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json, time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# CONFIG \n",
    "CSV_PATH = \"preprocessed_humor_data.csv\"\n",
    "RANDOM_STATE = 42\n",
    "RATE_LIMIT_S = 0.25            # ~4 req/s\n",
    "LABELS = [\"funny\", \"not_funny\", \"dont_understand\"]\n",
    "TEXT_COL = \"joke_text\"\n",
    "AGE_COL = \"age_bin\"            # use \"age_bin\" (e.g., 18-25). Change to \"age\" if you prefer\n",
    "GENDER_COL = \"gender\"\n",
    "ETH_COL = \"ethnicity\"\n",
    "LABEL_COL = \"response\"         # gold labels already canonical in your dataset\n",
    "\n",
    "\n",
    "# Helper: normalize model output to our 3 labels\n",
    "def _normalize_pred_label(x: Any) -> str:\n",
    "    if x is None:\n",
    "        return \"dont_understand\"\n",
    "    s = str(x).strip().lower().replace(\"-\", \"_\")\n",
    "    if \"understand\" in s and (\"dont\" in s or \"don't\" in s or \"did not\" in s or \"didn't\" in s or \"do not\" in s):\n",
    "        return \"dont_understand\"\n",
    "    if \"not\" in s and \"funny\" in s:\n",
    "        return \"not_funny\"\n",
    "    if s in {\"funny\", \"not_funny\", \"dont_understand\"}:\n",
    "        return s\n",
    "    if \"funny\" in s:\n",
    "        return \"funny\"\n",
    "    return \"dont_understand\"\n",
    "\n",
    "def split_70_10_20(df: pd.DataFrame, label_col: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Row-level 70/10/20; stratified by label when possible.\"\"\"\n",
    "    y = df[label_col]\n",
    "    # 70 / 30\n",
    "    try:\n",
    "        train_df, temp_df = train_test_split(df, test_size=0.30, stratify=y, random_state=RANDOM_STATE)\n",
    "    except ValueError:\n",
    "        train_df, temp_df = train_test_split(df, test_size=0.30, shuffle=True, random_state=RANDOM_STATE)\n",
    "        print(\"[WARN] Stratified split (train/temp) failed; used non-stratified.\")\n",
    "    # 10 / 20 from the 30 => 1/3 vs 2/3\n",
    "    try:\n",
    "        val_df, test_df = train_test_split(\n",
    "            temp_df, test_size=2/3, stratify=temp_df[label_col], random_state=RANDOM_STATE\n",
    "        )\n",
    "    except ValueError:\n",
    "        val_df, test_df = train_test_split(temp_df, test_size=2/3, shuffle=True, random_state=RANDOM_STATE)\n",
    "        print(\"[WARN] Stratified split (val/test) failed; used non-stratified.\")\n",
    "    return train_df.reset_index(drop=True), val_df.reset_index(drop=True), test_df.reset_index(drop=True)\n",
    "\n",
    "def evaluate_macro_f1(df: pd.DataFrame) -> float:\n",
    "    \"\"\"Calls your classify_with_gemini_rest on each row and returns Macro-F1.\"\"\"\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        gold = str(row[LABEL_COL]).strip().lower()\n",
    "        if gold not in {\"funny\", \"not_funny\", \"dont_understand\"}:\n",
    "            continue  # skip any unexpected label just in case\n",
    "\n",
    "        joke      = \"\" if pd.isna(row[TEXT_COL]) else str(row[TEXT_COL])\n",
    "        age       = \"\" if pd.isna(row[AGE_COL])  else str(row[AGE_COL])\n",
    "        gender    = \"\" if pd.isna(row[GENDER_COL]) else str(row[GENDER_COL])\n",
    "        ethnicity = \"\" if pd.isna(row[ETH_COL]) else str(row[ETH_COL])\n",
    "\n",
    "       \n",
    "        # classify_with_gemini_rest(joke, age, gender, ethnicity, temperature=0.0, max_output_tokens=128)\n",
    "        try:\n",
    "            out: Dict[str, Any] = classify_with_gemini_rest(joke, age, gender, ethnicity)\n",
    "            pred = _normalize_pred_label(out.get(\"label\"))\n",
    "        except Exception:\n",
    "            pred = \"dont_understand\"\n",
    "        # -\n",
    "\n",
    "        y_true.append(gold)\n",
    "        y_pred.append(pred)\n",
    "\n",
    "        if RATE_LIMIT_S:\n",
    "            time.sleep(RATE_LIMIT_S)\n",
    "        if (i + 1) % 25 == 0:\n",
    "            print(f\"Processed {i+1}/{len(df)} rows\")\n",
    "\n",
    "    macro_f1 = float(f1_score(y_true, y_pred, average=\"macro\", labels=LABELS))\n",
    "    print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
    "    print(pd.DataFrame(confusion_matrix(y_true, y_pred, labels=LABELS), index=LABELS, columns=LABELS))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, labels=LABELS, digits=4))\n",
    "    return macro_f1\n",
    "\n",
    "#  MAIN \n",
    "# Load your dataset\n",
    "df_full = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# (Optional) drop rows missing essentials\n",
    "df_full = df_full.dropna(subset=[TEXT_COL, LABEL_COL]).reset_index(drop=True)\n",
    "\n",
    "# Split 70/10/20\n",
    "train_df, val_df, test_df = split_70_10_20(df_full, LABEL_COL)\n",
    "print(f\"Split sizes -> train: {len(train_df)} | val: {len(val_df)} | test: {len(test_df)}\")\n",
    "\n",
    "# \"Train with prompt\": the “training” is your fixed prompt inside classify_with_gemini_rest,\n",
    "# so we directly evaluate it on val/test.\n",
    "\n",
    "print(\"\\n=== Evaluating on Validation set ===\")\n",
    "val_macro = evaluate_macro_f1(val_df)\n",
    "print(f\"\\nValidation Macro-F1: {val_macro:.4f}\")\n",
    "\n",
    "print(\"\\n=== Evaluating on Test set ===\")\n",
    "test_macro = evaluate_macro_f1(test_df)\n",
    "print(f\"\\nTest Macro-F1: {test_macro:.4f}\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
